#!/usr/bin/env python3
"""
Response grader for evaluating query responses.
Grades responses from 0-100% using ChatGPT analysis.
"""

import os
import json
import sys
import importlib.util

if 'openai' in sys.modules:
    OpenAI = getattr(sys.modules['openai'], 'OpenAI', None)  # type: ignore
else:
    _openai_spec = importlib.util.find_spec("openai")
    if _openai_spec:
        from openai import OpenAI  # type: ignore
    else:
        OpenAI = None  # type: ignore


def grade_response(original_query, response_text, morphosource_results):
    """
    Grade a query response using ChatGPT.
    
    Args:
        original_query (str): The original user query
        response_text (str): The response generated by the system
        morphosource_results (dict): The MorphoSource API results
        
    Returns:
        dict: Contains grade (0-100), reasoning, and breakdown
    """
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        print("âœ— OPENAI_API_KEY not configured")
        return {
            "status": "error",
            "message": "OPENAI_API_KEY not configured"
        }

    if OpenAI is None:
        print("âœ— OpenAI package is not installed")
        return {
            "status": "error",
            "message": "OpenAI client library is not installed"
        }
    
    try:
        client = OpenAI(api_key=api_key)
        
        # Extract result count from morphosource_results
        result_count = 0
        if isinstance(morphosource_results, dict):
            result_count = morphosource_results.get('count', 0)
            if result_count == 0 and 'results' in morphosource_results:
                result_count = len(morphosource_results.get('results', []))
        
        # Create grading prompt
        grading_prompt = f"""You are an expert evaluator for MorphoSource database query responses. 
Your task is to grade the quality of a response to a user's query on a scale of 0-100.

Grading Criteria:
- 0-20%: Failed to generate a valid query or response. No useful information provided.
- 21-40%: Generated a query but no results, or response is mostly incorrect/unhelpful.
- 41-60%: Generated some results but response is incomplete, contains errors, or misses key information.
- 61-80%: Good response with relevant information, minor issues or could be more comprehensive.
- 81-100%: Excellent response - accurate, comprehensive, well-formatted, directly answers the query.

Consider these factors:
1. Query Formation (0-25 points): Was a valid MorphoSource API query generated?
2. Results Quality (0-25 points): Did the query return relevant results?
3. Response Accuracy (0-25 points): Is the response factually accurate based on the data?
4. Response Completeness (0-25 points): Does the response fully address the user's question?

Original Query: {original_query}

Number of Results Returned: {result_count}

System Response:
{response_text}

Please provide your evaluation in the following JSON format:
{{
    "overall_grade": <0-100>,
    "query_formation": <0-25>,
    "results_quality": <0-25>,
    "response_accuracy": <0-25>,
    "response_completeness": <0-25>,
    "strengths": "<brief description of what was done well>",
    "weaknesses": "<brief description of issues or areas for improvement>",
    "reasoning": "<brief explanation of the overall grade>"
}}

Respond ONLY with the JSON object, no other text."""

        print("ðŸ“Š Grading response with ChatGPT...")
        
        completion = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert evaluator for database query responses. Always respond with valid JSON only."},
                {"role": "user", "content": grading_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        response_content = completion.choices[0].message.content.strip()
        
        # Parse JSON response
        try:
            # Remove markdown code blocks if present
            if response_content.startswith('```'):
                response_content = response_content.split('```')[1]
                if response_content.startswith('json'):
                    response_content = response_content[4:]
                response_content = response_content.strip()
            
            grade_data = json.loads(response_content)
            
            print(f"âœ“ Grade: {grade_data['overall_grade']}/100")
            print(f"  - Query Formation: {grade_data.get('query_formation', 0)}/25")
            print(f"  - Results Quality: {grade_data.get('results_quality', 0)}/25")
            print(f"  - Response Accuracy: {grade_data.get('response_accuracy', 0)}/25")
            print(f"  - Response Completeness: {grade_data.get('response_completeness', 0)}/25")
            
            return {
                "status": "success",
                "grade": grade_data['overall_grade'],
                "breakdown": {
                    "query_formation": grade_data.get('query_formation', 0),
                    "results_quality": grade_data.get('results_quality', 0),
                    "response_accuracy": grade_data.get('response_accuracy', 0),
                    "response_completeness": grade_data.get('response_completeness', 0)
                },
                "strengths": grade_data.get('strengths', ''),
                "weaknesses": grade_data.get('weaknesses', ''),
                "reasoning": grade_data.get('reasoning', ''),
                "result_count": result_count
            }
            
        except json.JSONDecodeError as e:
            print(f"âœ— Failed to parse grade response: {e}")
            print(f"Response was: {response_content}")
            return {
                "status": "error",
                "message": f"Failed to parse grading response: {e}",
                "raw_response": response_content
            }
        
    except Exception as e:
        print(f"âœ— Error grading response: {e}")
        return {
            "status": "error",
            "message": str(e)
        }


def main():
    """Main function to grade a response from command line args."""
    if len(sys.argv) < 4:
        print("Usage: grade_response.py <original_query> <response_file> <results_file>")
        sys.exit(1)
    
    original_query = sys.argv[1]
    response_file = sys.argv[2]
    results_file = sys.argv[3]
    
    # Read response text
    try:
        with open(response_file, 'r', encoding='utf-8') as f:
            response_data = json.load(f)
            response_text = response_data.get('response', '')
    except Exception as e:
        print(f"âœ— Error reading response file: {e}")
        response_text = ""
    
    # Read MorphoSource results
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            morphosource_results = json.load(f)
    except Exception as e:
        print(f"âœ— Error reading results file: {e}")
        morphosource_results = {}
    
    # Grade the response
    grade_result = grade_response(original_query, response_text, morphosource_results)
    
    # Write result to file
    with open('grade_result.json', 'w', encoding='utf-8') as f:
        json.dump(grade_result, f, indent=2)
    
    # Also output to GitHub Actions
    if 'GITHUB_OUTPUT' in os.environ:
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            if grade_result.get('status') == 'success':
                f.write(f"grade={grade_result['grade']}\n")
                f.write(f"reasoning={grade_result['reasoning']}\n")
            else:
                f.write(f"grade=0\n")
                f.write(f"reasoning=Error: {grade_result.get('message', 'Unknown error')}\n")


if __name__ == '__main__':
    main()
